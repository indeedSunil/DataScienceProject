{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Foundation of Data Science Project\n",
    "# COVID-19 Data Analysis \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")  \n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['figure.titlesize'] = 18\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DATA UNDERSTANDING AND PREPROCESSING\n",
    "\n",
    "This section covers data loading, inspection, cleaning, and transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_inspect_data(file_path):\n",
    "   \n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(\"\\n--- Dataset Overview ---\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\nSummary statistics:\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load COVID-19 confirmed cases data\n",
    "covid_data = load_and_inspect_data('Datasets/confirmed.csv')\n",
    "\n",
    "# Filter for Nepal data\n",
    "nepal_data = covid_data[covid_data['Country/Region'] == 'Nepal']\n",
    "\n",
    "# Display Nepal data\n",
    "print(\"\\n--- Nepal Data ---\")\n",
    "display(nepal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Data Structure Analysis ---\")\n",
    "print(f\"Number of countries/regions: {covid_data['Country/Region'].nunique()}\")\n",
    "print(f\"Number of provinces/states: {covid_data['Province/State'].nunique()}\")\n",
    "print(f\"Date range: From {covid_data.columns[4]} to {covid_data.columns[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Types of data: Structured, unstructured, semi-structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This dataset is STRUCTURED data in tabular format with:\")\n",
    "print(\"- Geographic identifiers (Country/Region, Province/State)\")\n",
    "print(\"- Geospatial coordinates (Lat, Long)\")\n",
    "print(\"- Time series data (daily confirmed cases)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2-3.4 Data Preprocessing & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_covid_data(df):\n",
    "    \"\"\"\n",
    "    Preprocessing  COVID-19 data by:\n",
    "    1. Reshaping from wide to long format\n",
    "    2. Converting date columns to proper datetime\n",
    "    3. Handling missing values\n",
    "    4. Creating aggregated country-level data\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing COVID-19 data...\")\n",
    "    \n",
    "    preprocessed_df = df.copy()\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    preprocessed_df.drop(columns=['Province/State', 'Lat', 'Long'], inplace=True)\n",
    "    #here inplace=True is used to modify the original DataFrame without creating a copy\n",
    "    \n",
    "    # 1. Check and handle missing values\n",
    "    print(f\"\\nMissing values before cleaning:\")\n",
    "    print(preprocessed_df.isnull().sum().sum())\n",
    "    \n",
    "    # The first .sum() computes the sum of missing values for each column, and the second .sum() computes the total across all columns.\n",
    "    \n",
    "    # 2. Reshape data from wide to long format\n",
    "    print(\"\\nReshaping data from wide to long format...\")\n",
    "    geo_cols = ['Country/Region']\n",
    "    \n",
    "    date_cols = preprocessed_df.columns[1:]  # everything after 'Country/Region'\n",
    "    long_df = pd.melt(\n",
    "        preprocessed_df, \n",
    "        id_vars=geo_cols, #geo cols to keep as identifiers and remain unchanged\n",
    "        value_vars=date_cols, \n",
    "        var_name='Date',\n",
    "        value_name='Confirmed'\n",
    "    )\n",
    "    \n",
    "    # 3. Convert date strings to datetime objects 01/22/20 type formatting\n",
    "    long_df['Date'] = pd.to_datetime(long_df['Date'], format='%m/%d/%y')\n",
    "    \n",
    "    # 4. Ensure confirmed cases are numeric\n",
    "    long_df['Confirmed'] = pd.to_numeric(long_df['Confirmed'], errors='coerce').fillna(0).astype(int)\n",
    "        # Ensures the 'Confirmed' column contains numeric values:\n",
    "            # pd.to_numeric() converts values to numbers\n",
    "            # errors='coerce' turns problematic values into NaN\n",
    "            # fillna(0) replaces any NaN values with zero\n",
    "            # astype(int) converts everything to integers\n",
    "    \n",
    "    # 5. Create a country-level aggregated dataset\n",
    "    print(\"\\nCreating country-level aggregated dataset...\")\n",
    "    country_df = long_df.groupby(['Country/Region', 'Date'])['Confirmed'].sum().reset_index() # ensures one row per country per data\n",
    "    \n",
    "    # 6. Sort  countries and then by date in asceinding order and calculate daily new cases\n",
    "    country_df = country_df.sort_values(['Country/Region', 'Date'])\n",
    "    country_df['Daily_New_Cases'] = country_df.groupby('Country/Region')['Confirmed'].diff().fillna(0)\n",
    "    country_df['Daily_New_Cases'] = country_df['Daily_New_Cases'].apply(lambda x: max(0, x)) # ensures no negative values\n",
    "    \n",
    "    # 7. Calculate rolling averages of 7 days . meaning average in like day(0) day(0,1 ) day(0,1,2) day(0,1,2,3) day(0,1,2,3,4) day(0,1,2,3,4,5) day(0,1,2,3,4,5,6) and so on.\n",
    "    country_df['7_Day_Avg_New_Cases'] = country_df.groupby('Country/Region')['Daily_New_Cases'].transform(\n",
    "        lambda x: x.rolling(7, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    print(\"\\nData preprocessing completed!\")\n",
    "    print(f\"Long format shape: {long_df.shape}\")\n",
    "    print(f\"Country-level aggregated data shape: {country_df.shape}\")\n",
    "    \n",
    "    return long_df, country_df\n",
    "\n",
    "# Apply preprocessing\n",
    "covid_long_df, covid_country_df = preprocess_covid_data(covid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the preprocessed data\n",
    "print(\"--- Preprocessed Data Sample (Long Format) ---\")\n",
    "display(covid_long_df.head())\n",
    "\n",
    "print(\"\\n--- Preprocessed Data Sample (Country-Level) - Nepal ---\")\n",
    "display(covid_country_df[covid_country_df['Country/Region'] == 'Nepal'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5-3.6 Data Wrangling, Enrichment & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_covid_data(df):\n",
    "    \"\"\"\n",
    "    Enrich COVID-19 data with additional features:\n",
    "    1. Extract month and year for temporal analysis\n",
    "    2. Calculate metrics like infection rate and growth rate\n",
    "    3. Add categorical variables for high/medium/low infection periods\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Preprocessed country-level COVID-19 dataframe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Enriched dataframe\n",
    "    \"\"\"\n",
    "    print(\"Enriching COVID-19 data with additional features...\")\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    enriched_df = df.copy()\n",
    "    \n",
    "    # 1. Extract temporal features like separate year month week and day of week\n",
    "    enriched_df['Year'] = enriched_df['Date'].dt.year\n",
    "    enriched_df['Month'] = enriched_df['Date'].dt.month\n",
    "    enriched_df['Week'] = enriched_df['Date'].dt.isocalendar().week\n",
    "    enriched_df['DayOfWeek'] = enriched_df['Date'].dt.dayofweek\n",
    "    \n",
    "    # 2. Calculate infection rate (rolling 7-day average of new cases)\n",
    "    # it is different from the 7-day average of confirmed cases as it calculates for all the column. not grouped by country\n",
    "    enriched_df['Infection_Rate'] = enriched_df['Daily_New_Cases'].rolling(7).mean()\n",
    "    \n",
    "    # 3. Calculate growth rate (percentage change)\n",
    "    # Fixing the error by applying the growth rate calculation correctly\n",
    "    enriched_df['Growth_Rate'] = enriched_df.groupby('Country/Region')['Confirmed'].apply(\n",
    "        lambda x: x.replace(0, np.nan).pct_change().replace([np.inf, -np.inf], np.nan).fillna(0) * 100\n",
    "    ).reset_index(level=0, drop=True)  # Reset index to align with the original dataframe\n",
    "             # x.replace(0, np.nan) - Replaces 0 values with NaN to avoid division by zero\n",
    "             # pct_change() - Calculates percentage change\n",
    "             # replace([np.inf, -np.inf], np.nan) - Replaces infinite values with NaN\n",
    "             # fillna(0) - Replaces NaN with 0\n",
    "             # * 100 - Converts to percentage\n",
    "\n",
    "\n",
    "\n",
    "    # 4. Calculate infection ratio (new cases divided by total confirmed cases)\n",
    "    enriched_df['Infection_Ratio'] = enriched_df['Daily_New_Cases'] / enriched_df['Confirmed'].replace(0, 1)\n",
    "    \n",
    "    # 5. Add categorical variables (based on 7-day average of new cases)\n",
    "    conditions = [\n",
    "        (enriched_df['7_Day_Avg_New_Cases'] == 0),\n",
    "        (enriched_df['7_Day_Avg_New_Cases'] > 0) & (enriched_df['7_Day_Avg_New_Cases'] <= 100),\n",
    "        (enriched_df['7_Day_Avg_New_Cases'] > 100) & (enriched_df['7_Day_Avg_New_Cases'] <= 1000),\n",
    "        (enriched_df['7_Day_Avg_New_Cases'] > 1000)\n",
    "    ]\n",
    "    choices = ['No Cases', 'Low', 'Medium', 'High']\n",
    "    enriched_df['Infection_Level'] = np.select(conditions, choices, default='Unknown')\n",
    "    \n",
    "    # 6. Validate data: Check for missing values\n",
    "    missing_values = enriched_df.isnull().sum()\n",
    "    print(\"\\nMissing values after enrichment:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "    \n",
    "    # Fill missing values with 0 for numeric columns\n",
    "    numeric_cols = enriched_df.select_dtypes(include=[np.number]).columns\n",
    "    enriched_df[numeric_cols] = enriched_df[numeric_cols].fillna(0)\n",
    "    \n",
    "    print(\"\\nData enrichment completed!\")\n",
    "    return enriched_df\n",
    "\n",
    "# Enrich the country-level data\n",
    "covid_enriched_df = enrich_covid_data(covid_country_df)\n",
    "\n",
    "# Display the enriched data\n",
    "print(\"\\n--- Enriched Data Sample ---\")\n",
    "display(covid_enriched_df.head())\n",
    "\n",
    "# Display the enriched data for Nepal only\n",
    "print(\"\\n--- Enriched Data Sample (Nepal) ---\")\n",
    "display(covid_enriched_df[covid_enriched_df['Country/Region'] == 'Nepal'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7-3.8 Data Transformation & Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 20 countries by total confirmed cases\n",
    "top_countries = covid_enriched_df.groupby('Country/Region')['Confirmed'].max().sort_values(ascending=False).head(20).index\n",
    "# covid_enriched_df.groupby('Country/Region')['Confirmed'] - Groups the data by country and selects the 'Confirmed' column\n",
    "# .max() - Gets the maximum value for each country (which represents their peak or most recent total case count)\n",
    "# .sort_values(ascending=False) - Sorts countries from highest to lowest case counts\n",
    "# .head(20) - Takes only the top 20 countries\n",
    "# .index - Extracts just the country names\n",
    "top_countries_df = covid_enriched_df[covid_enriched_df['Country/Region'].isin(top_countries)].copy() # create a boolean mask that's true only for rows where the countryt is in our top 20 list and the filtered data is copied to a new data frame to avoid modifying the original data\n",
    "\n",
    "print(f\"Selected top 20 countries by total confirmed cases:\")\n",
    "print(top_countries.tolist())\n",
    "\n",
    "# Normalize data for comparison across countries\n",
    "print(\"\\nNormalizing data for comparison across countries...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_for_comparison(df, countries, target_columns):\n",
    "    \"\"\"\n",
    "    Normalize data for each country to enable fair comparison\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Enriched dataframe\n",
    "    countries : list\n",
    "        List of countries to normalize\n",
    "    target_columns : list\n",
    "        List of columns to normalize\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Dataframe with normalized columns\n",
    "    \"\"\"\n",
    "    normalized_df = df[df['Country/Region'].isin(countries)].copy() # This creates a copy with only those countries' data and initializes the MinMaxScaler.\n",
    "    \n",
    "    # Create a scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Normalize each column for each country\n",
    "    for country in countries:\n",
    "        country_mask = normalized_df['Country/Region'] == country\n",
    "        for column in target_columns:\n",
    "            # Get the values for this country and column\n",
    "            values = normalized_df.loc[country_mask, column].values\n",
    "            \n",
    "            # Replace infinities and NaNs with 0\n",
    "            values = np.nan_to_num(values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "            # Skip if all values are the same (would cause division by zero in scaling)\n",
    "            if len(values) == 0 or np.min(values) == np.max(values):\n",
    "                normalized_df.loc[country_mask, f'Normalized_{column}'] = 0.5  # Set to middle value\n",
    "                continue\n",
    "                \n",
    "            # Reshape for the scaler and transform\n",
    "            values = values.reshape(-1, 1)\n",
    "            try:\n",
    "                normalized_values = scaler.fit_transform(values)\n",
    "                normalized_df.loc[country_mask, f'Normalized_{column}'] = normalized_values\n",
    "            except ValueError:\n",
    "                # If scaling fails, manually normalize to 0-1 range\n",
    "                min_val = np.min(values)\n",
    "                max_val = np.max(values)\n",
    "                if min_val == max_val:\n",
    "                    normalized_values = np.zeros(values.shape)\n",
    "                else:\n",
    "                    normalized_values = (values - min_val) / (max_val - min_val)\n",
    "                normalized_df.loc[country_mask, f'Normalized_{column}'] = normalized_values\n",
    "    \n",
    "    return normalized_df\n",
    "\n",
    "# Columns to normalize\n",
    "columns_to_normalize = ['Confirmed', 'Daily_New_Cases', '7_Day_Avg_New_Cases', 'Growth_Rate']\n",
    "\n",
    "# Fix any problematic values in the source data\n",
    "for column in columns_to_normalize:\n",
    "    top_countries_df[column] = top_countries_df[column].replace([np.inf, -np.inf], np.nan)\n",
    "    top_countries_df[column] = top_countries_df[column].fillna(0)\n",
    "\n",
    "# Apply normalization\n",
    "try:\n",
    "    normalized_df = normalize_for_comparison(top_countries_df, top_countries, columns_to_normalize)\n",
    "    print(\"\\n--- Normalized Data Sample ---\")\n",
    "    print(normalized_df.head().to_string())  # Using print instead of display\n",
    "except Exception as e:\n",
    "    print(f\"Error during normalization: {e}\")\n",
    "    # Create a simplified version without normalization as a fallback\n",
    "    normalized_df = top_countries_df.copy()\n",
    "    print(\"\\n--- Data Sample (normalization failed) ---\")\n",
    "    print(normalized_df.head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 4: DATA ANALYSIS\n",
    "\n",
    "This section covers descriptive analytics, exploratory data analysis, and data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1-4.2 Descriptive Analytics & Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_descriptive_analysis(df, column_name='Confirmed'):\n",
    "    \"\"\"\n",
    "    Perform descriptive statistical analysis on a column\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to analyze\n",
    "    column_name : str\n",
    "        Column to analyze\n",
    "    \"\"\"\n",
    "    print(f\"\\nDescriptive statistics for {column_name}:\")\n",
    "    stats_df = df.groupby('Country/Region')[column_name].describe()\n",
    "    # groups that data by country , for each country calculates summary statitics....\n",
    "    #here the describe() method is used to get summary statistics for the specified column (like mean, std, min, 25%, 50%, 75%, max) for each country.\n",
    "    print(stats_df.head().to_string())  # Using print instead of display\n",
    "    \n",
    "    # Additional statistics\n",
    "    print(\"\\nSkewness and Kurtosis (indicating distribution shape):\")\n",
    "    \n",
    "    # Using agg with lambda for each country\n",
    "    def calculate_skew_kurt(group):\n",
    "        from scipy.stats import skew, kurtosis\n",
    "        return pd.Series({ # panda series is like a single row of data\n",
    "            'skew': skew(group),\n",
    "            'kurtosis': kurtosis(group)\n",
    "        })\n",
    "    \n",
    "    skew_kurt = df.groupby('Country/Region')[column_name].apply(calculate_skew_kurt) # generates a new DataFrame with skewness and kurtosis for each country\n",
    "    print(skew_kurt.head().to_string())  # Using print instead of display\n",
    "\n",
    "# Perform descriptive analysis for confirmed cases and daily new cases\n",
    "perform_descriptive_analysis(covid_enriched_df, 'Confirmed')\n",
    "perform_descriptive_analysis(covid_enriched_df, 'Daily_New_Cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal analysis - how cases evolved over time\n",
    "print(\"\\nTemporal analysis - Monthly averages:\")\n",
    "monthly_avg = covid_enriched_df.groupby(['Year', 'Month'])['Daily_New_Cases'].mean().reset_index()\n",
    "display(monthly_avg.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3-4.5 Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_global_trend():\n",
    "    \"\"\"Plot the global COVID-19 trend over time\"\"\"\n",
    "    \n",
    "    # Calculate global daily cases\n",
    "    global_daily = covid_enriched_df.groupby('Date')['Daily_New_Cases'].sum().reset_index()\n",
    "    global_total = covid_enriched_df.groupby('Date')['Confirmed'].sum().reset_index()\n",
    "\n",
    "            # Group the COVID data by date\n",
    "            # Sum up the daily new cases across all countries for each date (into global_daily)\n",
    "            # Sum up the total confirmed cases across all countries for each date (into global_total)\n",
    "            # reset_index() converts the result back to a regular dataframe with 'Date' as a column\n",
    "\n",
    "            # For example:\n",
    "            # Date         Daily_New_Cases\n",
    "            # 2020-01-01   100\n",
    "            # 2020-01-02   150\n",
    "    \n",
    "    # Create a figure with two y-axes\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Plot daily new cases on the first y-axis\n",
    "    ax1.set_xlabel('Date', fontsize=14)\n",
    "    ax1.set_ylabel('Daily New Cases', color='tab:blue', fontsize=14)\n",
    "    ax1.bar(global_daily['Date'], global_daily['Daily_New_Cases'], \n",
    "            alpha=0.3, color='tab:blue', label='Daily New Cases')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    \n",
    "    # Create a 7-day rolling average\n",
    "    global_daily['7_Day_Avg'] = global_daily['Daily_New_Cases'].rolling(7).mean()\n",
    "    ax1.plot(global_daily['Date'], global_daily['7_Day_Avg'], \n",
    "             color='navy', linewidth=2, label='7-Day Moving Average')\n",
    "    \n",
    "    # Create a second y-axis for total confirmed cases\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Total Confirmed Cases', color='tab:red', fontsize=14)\n",
    "    ax2.plot(global_total['Date'], global_total['Confirmed'], \n",
    "             color='tab:red', linewidth=2, label='Total Confirmed Cases')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "    \n",
    "    # Combine legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    plt.title('Global COVID-19 Trend Over Time', fontsize=18)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Execute the visualization function\n",
    "plot_global_trend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_countries_comparison():\n",
    "    \"\"\"Plot comparison of COVID-19 cases across top countries\"\"\"\n",
    "    \n",
    "    # Get top 10 countries by total confirmed cases\n",
    "    top_10 = covid_enriched_df.groupby('Country/Region')['Confirmed'].max().sort_values(ascending=False).head(10)\n",
    "    \n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.barplot(x=top_10.values, y=top_10.index, palette='viridis')\n",
    "    \n",
    "    plt.title('Top 10 Countries by Total Confirmed COVID-19 Cases', fontsize=18)\n",
    "    plt.xlabel('Total Confirmed Cases', fontsize=14)\n",
    "    plt.ylabel('Country', fontsize=14)\n",
    "    plt.grid(True, axis='x', alpha=0.3)\n",
    "    plt.ticklabel_format(style='plain', axis='x')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Execute the visualization function\n",
    "plot_top_countries_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_infection_curves():\n",
    "    \"\"\"Plot normalized infection curves for top countries\"\"\"\n",
    "    \n",
    "    # Select top 6 countries for clarity\n",
    "    top_6_countries = normalized_df['Country/Region'].value_counts().head(10).index\n",
    "    \n",
    "    # Plot normalized confirmed cases\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    for country in top_6_countries:\n",
    "        country_data = normalized_df[normalized_df['Country/Region'] == country]\n",
    "        plt.plot(country_data['Date'], country_data['Normalized_Confirmed'], \n",
    "                 label=country, linewidth=2)\n",
    "    \n",
    "    plt.title('Normalized COVID-19 Infection Curves for Top 10 Countries', fontsize=18)\n",
    "    plt.xlabel('Date', fontsize=14)\n",
    "    plt.ylabel('Normalized Confirmed Cases (0-1 scale)', fontsize=14)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Execute the visualization function\n",
    "plot_infection_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap():\n",
    "    \"\"\"Plot heatmap of COVID-19 cases by month for top countries\"\"\"\n",
    "    \n",
    "    # Prepare data for heatmap - monthly cases for top countries\n",
    "    top_10_countries = covid_enriched_df.groupby('Country/Region')['Confirmed'].max().sort_values(ascending=False).head(10).index\n",
    "    monthly_cases = covid_enriched_df[covid_enriched_df['Country/Region'].isin(top_10_countries)]\n",
    "    \n",
    "    # Create a pivot table: Countries x Months with avg daily cases as values\n",
    "    heatmap_data = pd.pivot_table(\n",
    "        monthly_cases,\n",
    "        values='Daily_New_Cases',\n",
    "        index='Country/Region',\n",
    "        columns=['Year', 'Month'],\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    sns.heatmap(heatmap_data, cmap='YlOrRd', annot=False, linewidths=.5)\n",
    "    \n",
    "    plt.title('Average Daily New COVID-19 Cases by Month and Country', fontsize=18)\n",
    "    plt.xlabel('Year and Month', fontsize=14)\n",
    "    plt.ylabel('Country', fontsize=14)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Execute the visualization function\n",
    "plot_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Feature Engineering for Advanced Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_pandemic_phases(df):\n",
    "    \"\"\"\n",
    "    Extract key pandemic phases based on global trends\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Enriched COVID dataframe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with pandemic phases\n",
    "    \"\"\"\n",
    "    # Calculate global daily cases\n",
    "    global_daily = df.groupby('Date')['Daily_New_Cases'].sum().reset_index()\n",
    "    \n",
    "    # Calculate the global 7-day average\n",
    "    global_daily['Global_7Day_Avg'] = global_daily['Daily_New_Cases'].rolling(7).mean()\n",
    "    \n",
    "    # Find peaks and troughs to identify waves\n",
    "    # This is a simplified approach - in reality, more sophisticated algorithms would be used\n",
    "    \n",
    "    # Calculate the difference between consecutive days\n",
    "    global_daily['diff'] = global_daily['Global_7Day_Avg'].diff()\n",
    "    \n",
    "    # Identify sign changes (from positive to negative) to find peaks\n",
    "    global_daily['sign_change'] = np.sign(global_daily['diff']) != np.sign(global_daily['diff'].shift(1))\n",
    "    \n",
    "    # Identify potential waves (peaks)\n",
    "    peaks = global_daily[(global_daily['sign_change']) & (global_daily['diff'] < 0)]\n",
    "    \n",
    "    # Only keep significant peaks (those with substantial cases)\n",
    "    significant_peaks = peaks[peaks['Global_7Day_Avg'] > peaks['Global_7Day_Avg'].quantile(0.75)]\n",
    "    \n",
    "    print(f\"\\nIdentified {len(significant_peaks)} significant COVID-19 waves globally\")\n",
    "    print(\"Peak dates:\")\n",
    "    display(significant_peaks[['Date', 'Global_7Day_Avg']].sort_values('Global_7Day_Avg', ascending=False).head())\n",
    "    \n",
    "    # Define pandemic phases based on significant peaks\n",
    "    if len(significant_peaks) >= 3:\n",
    "        phase_boundaries = significant_peaks.nlargest(3, 'Global_7Day_Avg')['Date'].sort_values().tolist()\n",
    "        \n",
    "        # Function to assign phases\n",
    "        def assign_phase(date):\n",
    "            if date < phase_boundaries[0]:\n",
    "                return \"First Wave\"\n",
    "            elif date < phase_boundaries[1]:\n",
    "                return \"Second Wave\"\n",
    "            elif date < phase_boundaries[2]:\n",
    "                return \"Third Wave\" \n",
    "            else:\n",
    "                return \"Later Waves\"\n",
    "        \n",
    "        # Add phase information to the original dataframe\n",
    "        df['Pandemic_Phase'] = df['Date'].apply(assign_phase)\n",
    "        \n",
    "        print(\"\\nPandemic phases assigned based on global peaks\")\n",
    "    else:\n",
    "        print(\"\\nInsufficient data to identify multiple pandemic waves\")\n",
    "        df['Pandemic_Phase'] = \"Undetermined\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add pandemic phases to the enriched dataframe\n",
    "covid_enriched_df = extract_key_pandemic_phases(covid_enriched_df)\n",
    "\n",
    "# Display the distribution of data across pandemic phases\n",
    "print(\"\\nData distribution across pandemic phases:\")\n",
    "display(covid_enriched_df['Pandemic_Phase'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_peak_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculate peak metrics for each country\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Enriched COVID dataframe with pandemic phases\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with peak metrics by country\n",
    "    \"\"\"\n",
    "    # Group by country\n",
    "    countries = df['Country/Region'].unique()\n",
    "    peak_metrics = []\n",
    "    \n",
    "    for country in countries:\n",
    "        country_data = df[df['Country/Region'] == country].copy()\n",
    "        \n",
    "        # Skip countries with limited data\n",
    "        if len(country_data) < 30:\n",
    "            continue\n",
    "            \n",
    "        # Find peak daily cases\n",
    "        peak_day = country_data.loc[country_data['Daily_New_Cases'].idxmax()]\n",
    "        peak_daily = peak_day['Daily_New_Cases']\n",
    "        peak_date = peak_day['Date']\n",
    "        \n",
    "        # Find peak 7-day average\n",
    "        peak_avg = country_data['7_Day_Avg_New_Cases'].max()\n",
    "        peak_avg_date = country_data.loc[country_data['7_Day_Avg_New_Cases'].idxmax(), 'Date']\n",
    "        \n",
    "        # Calculate days to peak from first case\n",
    "        first_case_date = country_data.loc[country_data['Confirmed'] > 0, 'Date'].min()\n",
    "        if pd.notna(first_case_date) and pd.notna(peak_date):\n",
    "            days_to_peak = (peak_date - first_case_date).days\n",
    "        else:\n",
    "            days_to_peak = None\n",
    "            \n",
    "        # Calculate growth metrics\n",
    "        max_growth_rate = country_data['Growth_Rate'].max()\n",
    "        \n",
    "        # Store metrics\n",
    "        peak_metrics.append({\n",
    "            'Country/Region': country,\n",
    "            'Peak_Daily_Cases': peak_daily,\n",
    "            'Peak_Date': peak_date,\n",
    "            'Peak_7Day_Avg': peak_avg,\n",
    "            'Peak_Avg_Date': peak_avg_date,\n",
    "            'Days_To_Peak': days_to_peak,\n",
    "            'Max_Growth_Rate': max_growth_rate\n",
    "        })\n",
    "    \n",
    "    # Create dataframe of peak metrics\n",
    "    peak_metrics_df = pd.DataFrame(peak_metrics)\n",
    "    \n",
    "    print(\"\\nCalculated peak metrics for each country:\")\n",
    "    display(peak_metrics_df.head())\n",
    "    \n",
    "    return peak_metrics_df\n",
    "\n",
    "# Calculate peak metrics for each country\n",
    "peak_metrics_df = calculate_peak_metrics(covid_enriched_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_days_to_peak():\n",
    "    \"\"\"Plot days to peak for top countries\"\"\"\n",
    "    \n",
    "    # Select top 20 countries with most cases for analysis\n",
    "    top_20_countries = peak_metrics_df.sort_values('Peak_Daily_Cases', ascending=False).head(20)\n",
    "    \n",
    "    # Create the bar chart\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(x='Days_To_Peak', y='Country/Region', \n",
    "                data=top_20_countries.sort_values('Days_To_Peak'), \n",
    "                palette='coolwarm')\n",
    "    \n",
    "    plt.title('Days from First Case to Peak COVID-19 Outbreak by Country', fontsize=18)\n",
    "    plt.xlabel('Number of Days', fontsize=14)\n",
    "    plt.ylabel('Country', fontsize=14)\n",
    "    plt.grid(True, axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Execute the visualization function\n",
    "plot_days_to_peak()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOINING WITH WORLD HAPPINESS REPORT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def load_and_preprocess_happiness_data(file_path='Datasets/worldwide_happiness_report.csv'):\n",
    "    \"\"\"\n",
    "    Load and preprocess World Happiness Report data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the World Happiness Report CSV\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Preprocessed World Happiness dataframe\n",
    "    \"\"\"\n",
    "    print(\"\\nLoading World Happiness Report data...\")\n",
    "    \n",
    "   \n",
    "        # Try to load the actual file if available\n",
    "    happiness_df = pd.read_csv(file_path)\n",
    "    print(\"Successfully loaded World Happiness Report data\")\n",
    "   \n",
    "    \n",
    "    # Display the data\n",
    "    print(\"\\n--- World Happiness Report Data Sample ---\")\n",
    "    print(happiness_df.head().to_string())\n",
    "   \n",
    "    \n",
    "    return happiness_df\n",
    "\n",
    "# Load happiness data\n",
    "happiness_df = load_and_preprocess_happiness_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_covid_and_happiness_data(covid_df, happiness_df, peak_metrics_df):\n",
    "    \"\"\"\n",
    "    Join COVID-19 data with World Happiness Report data for correlation analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    covid_df : pd.DataFrame\n",
    "        Enriched COVID-19 dataframe\n",
    "    happiness_df : pd.DataFrame\n",
    "        World Happiness Report dataframe\n",
    "    peak_metrics_df : pd.DataFrame\n",
    "        Peak metrics by country\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Joined dataframe for analysis\n",
    "    \"\"\"\n",
    "    print(\"\\nJoining COVID-19 data with World Happiness Report data...\")\n",
    "    \n",
    "    # Ensure country names are compatible between datasets\n",
    "    covid_countries = set(covid_df['Country/Region'].unique())\n",
    "    happiness_countries = set(happiness_df['Country'].unique())\n",
    "    \n",
    "    print(f\"\\nCOVID-19 dataset has {len(covid_countries)} countries\")\n",
    "    print(f\"World Happiness Report has {len(happiness_countries)} countries\")\n",
    "    print(f\"Common countries: {len(covid_countries.intersection(happiness_countries))}\")\n",
    "    \n",
    "    # Create a mapping for inconsistent country names\n",
    "    country_mapping = {\n",
    "        'US': 'United States',\n",
    "        'Korea, South': 'South Korea',\n",
    "        'United Kingdom': 'UK',\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "    \n",
    "    # Apply country name standardization\n",
    "    covid_summary = covid_df.copy()\n",
    "    covid_summary['Country_Std'] = covid_summary['Country/Region'].map(\n",
    "        lambda x: country_mapping.get(x, x))\n",
    "    \n",
    "    # Create a dataframe with summary COVID metrics per country\n",
    "    covid_summary = covid_summary.groupby('Country_Std').agg({\n",
    "        'Confirmed': 'max',\n",
    "        'Daily_New_Cases': 'max',\n",
    "        '7_Day_Avg_New_Cases': 'max'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Merge the peak metrics\n",
    "    if peak_metrics_df is not None:\n",
    "        # Standardize country names in peak metrics\n",
    "        peak_metrics_copy = peak_metrics_df.copy()\n",
    "        peak_metrics_copy['Country_Std'] = peak_metrics_copy['Country/Region'].map(\n",
    "            lambda x: country_mapping.get(x, x))\n",
    "        \n",
    "        covid_summary = covid_summary.merge(\n",
    "            peak_metrics_copy[['Country_Std', 'Peak_7Day_Avg', 'Days_To_Peak', 'Max_Growth_Rate']], \n",
    "            on='Country_Std', \n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "    # Merge with happiness data\n",
    "    # Standardize happiness country names for better matching\n",
    "    happiness_copy = happiness_df.copy()\n",
    "    happiness_copy['Country_Std'] = happiness_copy['Country'].map(\n",
    "        lambda x: country_mapping.get(x, x))\n",
    "    \n",
    "    # Perform the join\n",
    "    joined_df = covid_summary.merge(\n",
    "        happiness_copy,\n",
    "        left_on='Country_Std',\n",
    "        right_on='Country_Std',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Clean up the joined dataframe\n",
    "    if 'Country_x' in joined_df.columns and 'Country_y' in joined_df.columns:\n",
    "        joined_df = joined_df.rename(columns={'Country_y': 'Country'})\n",
    "        joined_df = joined_df.drop(columns=['Country_x'])\n",
    "    \n",
    "    print(f\"\\nSuccessfully joined data for {joined_df.shape[0]} countries\")\n",
    "    print(\"\\n--- Joined Data Sample ---\")\n",
    "    print(joined_df.head().to_string())\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "# Join the datasets\n",
    "joined_df = join_covid_and_happiness_data(covid_enriched_df, happiness_df, peak_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "\n",
    "def calculate_correlations(df):\n",
    "    \"\"\"\n",
    "    Calculate correlations between COVID-19 metrics and happiness factors\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Joined dataframe with COVID and happiness data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Correlation dataframe\n",
    "    \"\"\"\n",
    "    print(\"\\nCalculating correlations between COVID-19 metrics and happiness factors...\")\n",
    "    \n",
    "    # Select relevant columns for correlation analysis\n",
    "    covid_cols = ['Confirmed', 'Daily_New_Cases', 'Peak_7Day_Avg', 'Days_To_Peak', 'Max_Growth_Rate']\n",
    "    happiness_cols = ['Score', 'GDP_per_capita', 'Social_support', 'Healthy_life_expectancy', \n",
    "                     'Freedom_to_make_life_choices', 'Generosity', 'Perceptions_of_corruption']\n",
    "    \n",
    "    # Ensure all columns exist in the dataframe\n",
    "    covid_cols = [col for col in covid_cols if col in df.columns]\n",
    "    happiness_cols = [col for col in happiness_cols if col in df.columns]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_df = df[covid_cols + happiness_cols].corr()\n",
    "    \n",
    "    print(\"\\n--- Correlation Matrix ---\")\n",
    "    print(correlation_df.loc[covid_cols, happiness_cols].to_string())\n",
    "    \n",
    "    return correlation_df\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_df = calculate_correlations(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(corr_df):\n",
    "    \"\"\"Plot a heatmap of correlations\"\"\"\n",
    "    \n",
    "    # Select relevant sections of the correlation matrix\n",
    "    covid_cols = ['Confirmed', 'Daily_New_Cases', 'Peak_7Day_Avg', 'Days_To_Peak', 'Max_Growth_Rate']\n",
    "    happiness_cols = ['Score', 'GDP_per_capita', 'Social_support', 'Healthy _life_expectancy', \n",
    "                     'Freedom_to_make_life_choices', 'Generosity', 'Perceptions_of_corruption']\n",
    "    #here score is happiness score\n",
    "    \n",
    "    # Ensure all columns exist in the dataframe\n",
    "    covid_cols = [col for col in covid_cols if col in corr_df.index]\n",
    "    happiness_cols = [col for col in happiness_cols if col in corr_df.columns]\n",
    "    \n",
    "    if not covid_cols or not happiness_cols:\n",
    "        print(\"Warning: Not enough data for correlation heatmap\")\n",
    "        return\n",
    "        \n",
    "    plot_corr = corr_df.loc[covid_cols, happiness_cols]\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(plot_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    \n",
    "    plt.title('Correlation between COVID-19 Metrics and Socioeconomic Factors', fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_scatter_relationships(df):\n",
    "    \"\"\"Plot scatter plots for key relationships\"\"\"\n",
    "    \n",
    "    # Check if we have the necessary columns\n",
    "    required_columns = ['GDP_per_capita', 'Confirmed', 'Healthy _life_expectancy', 'Days_To_Peak', \n",
    "                       'Social_support', 'Max_Growth_Rate', 'Score', 'Daily_New_Cases']\n",
    "    \n",
    "    # Filter to only include columns that exist in the dataframe\n",
    "    available_columns = [col for col in required_columns if col in df.columns]\n",
    "    \n",
    "    if len(available_columns) < 4:  # Need at least 4 columns for 2x2 plot\n",
    "        print(\"Warning: Not enough data columns for scatter plots\")\n",
    "        return\n",
    "    \n",
    "    # Create a subplot grid\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    axes = axes.flatten()  # Flatten for easier iteration\n",
    "    \n",
    "    # Define plot pairs (x, y, title)\n",
    "    plot_pairs = [\n",
    "        ('GDP_per_capita', 'Confirmed', 'GDP per Capita vs Total COVID-19 Cases'),\n",
    "        ('Healthy _life_expectancy', 'Days_To_Peak', 'Life Expectancy vs Days to Peak Infection'),\n",
    "        ('Social_support', 'Max_Growth_Rate', 'Social Support vs Maximum Growth Rate'),\n",
    "        ('Score', 'Daily_New_Cases', 'Happiness Score vs Peak Daily Cases')\n",
    "    ]\n",
    "    \n",
    "    # Create only the plots where we have both columns\n",
    "    plot_idx = 0\n",
    "    for i, (x_col, y_col, title) in enumerate(plot_pairs):\n",
    "        if x_col in df.columns and y_col in df.columns and plot_idx < len(axes):\n",
    "            ax = axes[plot_idx]\n",
    "            \n",
    "            # Create scatter plot\n",
    "            sns.scatterplot(x=x_col, y=y_col, data=df, ax=ax, alpha=0.7)\n",
    "            \n",
    "            # Add title and labels\n",
    "            ax.set_title(title, fontsize=14)\n",
    "            ax.set_xlabel(x_col.replace('_', ' '), fontsize=12)\n",
    "            ax.set_ylabel(y_col.replace('_', ' '), fontsize=12)\n",
    "            \n",
    "            # Add regression line if we have enough data points\n",
    "            if len(df) > 2:\n",
    "                x = df[x_col].values\n",
    "                y = df[y_col].values\n",
    "                if len(set(x)) > 1:  # Make sure x has at least 2 different values\n",
    "                    try:\n",
    "                        ax.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), color='red')\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not draw regression line for {title}: {e}\")\n",
    "            \n",
    "            plot_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot correlations and relationships\n",
    "try:\n",
    "    plot_correlation_heatmap(correlation_df)\n",
    "except Exception as e:\n",
    "    print(f\"Error plotting correlation heatmap: {e}\")\n",
    "\n",
    "try:\n",
    "    plot_scatter_relationships(joined_df)\n",
    "except Exception as e:\n",
    "    print(f\"Error plotting scatter relationships: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
